{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lokeshbudda/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lokeshbudda/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lokeshbudda/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/lokeshbudda/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Dense, Dropout, LSTM, Embedding, \n",
    "                                    Bidirectional, Conv1D, MaxPooling1D, \n",
    "                                    Input, concatenate, GlobalMaxPooling1D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner as kt\n",
    "import gradio as gr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load spaCy for advanced NLP features\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced text preprocessing\n",
    "def enhanced_clean_text(text):\n",
    "    \"\"\"Comprehensive text cleaning with multiple pattern removal and normalization\"\"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove MBTI type patterns\n",
    "    text = re.sub(r'\\b[A-Z]{4}\\b', '', text)\n",
    "    # Remove special characters except basic punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text)\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    # Correct spelling (basic correction)\n",
    "    # text = str(TextBlob(text).correct())\n",
    "    return text\n",
    "\n",
    "def advanced_tokenization(text):\n",
    "    # Process with spaCy for better lemmatization\n",
    "    doc = nlp(text)\n",
    "    # Keep only nouns, adjectives, verbs, and adverbs\n",
    "    allowed_pos = {'NOUN', 'ADJ', 'VERB', 'ADV'}\n",
    "    tokens = [token.lemma_ for token in doc if token.pos_ in allowed_pos and not token.is_stop]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def extract_text_features(df):\n",
    "    # Sentiment analysis\n",
    "    df['sentiment'] = df['cleaned'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    \n",
    "    # Readability scores\n",
    "    def flesch_reading_ease(text):\n",
    "        sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "        words = len(text.split())\n",
    "        syllables = sum([len(re.findall(r'[aeiouy]+', word.lower())) for word in text.split()])\n",
    "        if sentences == 0 or words == 0:\n",
    "            return 0\n",
    "        return 206.835 - 1.015*(words/sentences) - 84.6*(syllables/words)\n",
    "    \n",
    "    df['readability'] = df['cleaned'].apply(flesch_reading_ease)\n",
    "    \n",
    "    # Word and character counts\n",
    "    df['word_count'] = df['cleaned'].apply(lambda x: len(x.split()))\n",
    "    df['char_count'] = df['cleaned'].apply(len)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_data(max_seq_len=200):\n",
    "    df = pd.read_csv('../mbti_1.csv')\n",
    "    \n",
    "    # Enhanced cleaning\n",
    "    df['cleaned'] = df['posts'].apply(enhanced_clean_text)\n",
    "    \n",
    "    # Extract additional text features\n",
    "    df = extract_text_features(df)\n",
    "    \n",
    "    # Advanced tokenization\n",
    "    df['tokens'] = df['cleaned'].apply(advanced_tokenization)\n",
    "    \n",
    "    # Limit sequence length\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: x[:max_seq_len])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_glove_embeddings(file_path, vocab, embedding_dim=100):\n",
    "    embeddings = {}\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    \n",
    "    # Build embedding matrix with +1 for padding token\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, embedding_dim))\n",
    "    found = 0\n",
    "    for word, i in vocab.items():\n",
    "        if word in embeddings:\n",
    "            embedding_matrix[i] = embeddings[word]\n",
    "            found += 1\n",
    "        else:\n",
    "            # Initialize unknown words with random values\n",
    "            embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    \n",
    "    print(f\"Found embeddings for {found}/{len(vocab)} words ({100*found/len(vocab):.2f}%)\")\n",
    "    return embedding_matrix\n",
    "\n",
    "def prepare_data(df, max_seq_len=200):\n",
    "    all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "    vocab = {word: i+1 for i, word in enumerate(set(all_tokens))}\n",
    "    \n",
    "    # Convert tokens to sequences of indices\n",
    "    X_sequences = [[vocab.get(token, 0) for token in tokens] for tokens in df['tokens']]\n",
    "    X_padded = np.array([seq + [0]*(max_seq_len - len(seq)) if len(seq) < max_seq_len else seq[:max_seq_len] \n",
    "                         for seq in X_sequences])\n",
    "    \n",
    "    # Prepare MLP features (TF-IDF + SVD + additional features)\n",
    "    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    tfidf_features = tfidf.fit_transform(df['cleaned'])\n",
    "    svd = TruncatedSVD(n_components=100)\n",
    "    tfidf_svd = svd.fit_transform(tfidf_features)\n",
    "    \n",
    "    # Combine with additional features\n",
    "    additional_features = df[['sentiment', 'readability', 'word_count', 'char_count']].values\n",
    "    mlp_features = np.concatenate([tfidf_svd, additional_features], axis=1)\n",
    "    \n",
    "    # Normalize features\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    mlp_features = scaler.fit_transform(mlp_features)\n",
    "    \n",
    "    y = df['type']\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    y_categorical = to_categorical(y_encoded)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, X_mlp_train, X_mlp_test, y_train, y_test = train_test_split(\n",
    "        X_padded, mlp_features, y_categorical, test_size=0.2, stratify=y_encoded, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, X_mlp_train, X_mlp_test, y_train, y_test, vocab, le, tfidf, svd, scaler\n",
    "\n",
    "def build_hybrid_model(hp, vocab_size, embedding_matrix, embedding_dim=100, max_seq_len=200):\n",
    "    \"\"\"Build an enhanced hybrid CNN-LSTM-MLP model with hyperparameter tuning\"\"\"\n",
    "    # Sequence input branch (CNN-LSTM)\n",
    "    seq_input = Input(shape=(max_seq_len,))\n",
    "    \n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size + 1,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_seq_len,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=hp.Boolean('trainable_embedding', default=True)  # Now trainable\n",
    "    )(seq_input)\n",
    "    \n",
    "    # CNN layers with more options\n",
    "    conv1 = Conv1D(\n",
    "        filters=hp.Int('conv_filters1', 64, 256, step=64),\n",
    "        kernel_size=hp.Int('kernel_size1', 3, 7),\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_regularizer=l2(hp.Float('conv_l2', 1e-5, 1e-3, sampling='log'))\n",
    "    )(embedding_layer)\n",
    "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    \n",
    "    conv2 = Conv1D(\n",
    "        filters=hp.Int('conv_filters2', 32, 256, step=32),\n",
    "        kernel_size=hp.Int('kernel_size2', 2, 5),\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    )(pool1)\n",
    "    pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    \n",
    "    # Bidirectional LSTM with more options\n",
    "    lstm = Bidirectional(LSTM(\n",
    "        units=hp.Int('lstm_units', 64, 512, step=64),\n",
    "        return_sequences=False,\n",
    "        dropout=hp.Float('lstm_dropout', 0.1, 0.5),\n",
    "        recurrent_dropout=hp.Float('recurrent_dropout', 0.1, 0.3),\n",
    "        kernel_regularizer=l2(hp.Float('lstm_l2', 1e-5, 1e-3, sampling='log'))\n",
    "    ))(pool2)\n",
    "    \n",
    "    # MLP input branch\n",
    "    mlp_input = Input(shape=(104,))  # 100 (SVD) + 4 (additional features)\n",
    "    \n",
    "    # Enhanced MLP layers\n",
    "    mlp_layer = Dense(\n",
    "        units=hp.Int('mlp_units1', 128, 1024, step=128),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(hp.Float('mlp_l2', 1e-5, 1e-3, sampling='log'))\n",
    "    )(mlp_input)\n",
    "    mlp_layer = Dropout(hp.Float('mlp_dropout1', 0.2, 0.5))(mlp_layer)\n",
    "    \n",
    "    mlp_layer = Dense(\n",
    "        units=hp.Int('mlp_units2', 64, 512, step=64),\n",
    "        activation='relu'\n",
    "    )(mlp_layer)\n",
    "    mlp_layer = Dropout(hp.Float('mlp_dropout2', 0.1, 0.3))(mlp_layer)\n",
    "    \n",
    "    # Combine features with attention\n",
    "    combined = concatenate([lstm, mlp_layer])\n",
    "    \n",
    "    # Additional dense layers with skip connections\n",
    "    dense = Dense(\n",
    "        units=hp.Int('dense_units1', 256, 1024, step=128),\n",
    "        activation='relu'\n",
    "    )(combined)\n",
    "    dense = Dropout(hp.Float('dense_dropout1', 0.2, 0.5))(dense)\n",
    "    \n",
    "    dense = Dense(\n",
    "        units=hp.Int('dense_units2', 128, 512, step=64),\n",
    "        activation='relu'\n",
    "    )(dense)\n",
    "    dense = Dropout(hp.Float('dense_dropout2', 0.1, 0.3))(dense)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(16, activation='softmax')(dense)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[seq_input, mlp_input], outputs=output)\n",
    "    \n",
    "    # Enhanced optimizer with more options\n",
    "    optimizer = Adam(\n",
    "        learning_rate=hp.Choice('learning_rate', [1e-4, 3e-4, 1e-3, 3e-3]),\n",
    "        clipvalue=hp.Float('clipvalue', 0.1, 1.0),\n",
    "        beta_1=hp.Float('beta_1', 0.8, 0.99),\n",
    "        beta_2=hp.Float('beta_2', 0.9, 0.999)\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [01h 55m 45s]\n",
      "val_accuracy: 0.39769452810287476\n",
      "\n",
      "Best val_accuracy So Far: 0.4135446548461914\n",
      "Total elapsed time: 09h 03m 29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 85ms/step - accuracy: 0.4237 - loss: 1.8257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4081\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "No method for generating JsonSchema for core_schema.type='invalid' (expected: GenerateJsonSchema.invalid_schema)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/json_schema.py:336\u001b[0m, in \u001b[0;36mGenerateJsonSchema.build_schema_type_to_method\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     mapping[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_name)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GenerateJsonSchema' object has no attribute 'invalid_schema'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m     iface\u001b[38;5;241m.\u001b[39mlaunch()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 99\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[5], line 89\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {le\u001b[38;5;241m.\u001b[39mclasses_[i]: \u001b[38;5;28mfloat\u001b[39m(proba[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m16\u001b[39m)}\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Gradio interface\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m iface \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mInterface(\n\u001b[1;32m     90\u001b[0m     fn\u001b[38;5;241m=\u001b[39mpredict,\n\u001b[1;32m     91\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mTextbox(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your text\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     92\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mLabel(num_top_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     93\u001b[0m     title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnhanced MBTI Personality Predictor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model uses advanced NLP techniques to predict MBTI personality types from text.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m )\n\u001b[1;32m     96\u001b[0m iface\u001b[38;5;241m.\u001b[39mlaunch()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gradio/interface.py:508\u001b[0m, in \u001b[0;36mInterface.__init__\u001b[0;34m(self, fn, inputs, outputs, examples, cache_examples, cache_mode, examples_per_page, example_labels, live, title, description, article, theme, flagging_mode, flagging_options, flagging_dir, flagging_callback, analytics_enabled, batch, max_batch_size, api_name, _api_mode, allow_duplication, concurrency_limit, css, css_paths, js, head, head_paths, additional_inputs, additional_inputs_accordion, submit_btn, stop_btn, clear_btn, delete_cache, show_progress, fill_width, allow_flagging, time_limit, stream_every, deep_link, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflagging_callback\u001b[38;5;241m.\u001b[39msetup(\n\u001b[1;32m    503\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_components \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_components,\n\u001b[1;32m    504\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflagging_dir,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    505\u001b[0m         )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# Render the Gradio UI\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeep_link:\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeep_link\u001b[38;5;241m.\u001b[39mactivate()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gradio/blocks.py:2323\u001b[0m, in \u001b[0;36mBlocks.__exit__\u001b[0;34m(self, exc_type, *args)\u001b[0m\n\u001b[1;32m   2321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren)\n\u001b[0;32m-> 2323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_file()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapp \u001b[38;5;241m=\u001b[39m App\u001b[38;5;241m.\u001b[39mcreate_app(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   2326\u001b[0m     block_fn\u001b[38;5;241m.\u001b[39mtracks_progress \u001b[38;5;28;01mfor\u001b[39;00m block_fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfns\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   2327\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gradio/blocks.py:2271\u001b[0m, in \u001b[0;36mBlocks.get_config_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config_file\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BlocksConfigDict:\n\u001b[1;32m   2230\u001b[0m     config: BlocksConfigDict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2231\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: VERSION,\n\u001b[1;32m   2232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_prefix\u001b[39m\u001b[38;5;124m\"\u001b[39m: API_PREFIX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m   2270\u001b[0m     }\n\u001b[0;32m-> 2271\u001b[0m     config\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_config\u001b[38;5;241m.\u001b[39mget_config())  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_heartbeat\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mconnect_heartbeat(\n\u001b[1;32m   2273\u001b[0m         config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   2274\u001b[0m     )\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gradio/blocks.py:990\u001b[0m, in \u001b[0;36mBlocksConfig.get_config\u001b[0;34m(self, renderable)\u001b[0m\n\u001b[1;32m    986\u001b[0m blocks_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    988\u001b[0m )  \u001b[38;5;66;03m# freeze as list to prevent concurrent re-renders from changing the dict during loop, see https://github.com/gradio-app/gradio/issues/9991\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _id, block \u001b[38;5;129;01min\u001b[39;00m blocks_items:\n\u001b[0;32m--> 990\u001b[0m     block_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_for_block(_id, rendered_ids, block, renderable)\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block_config:\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gradio/blocks.py:934\u001b[0m, in \u001b[0;36mBlocksConfig.config_for_block\u001b[0;34m(_id, rendered_ids, block, renderable)\u001b[0m\n\u001b[1;32m    932\u001b[0m     block_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrenderable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m renderable\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block\u001b[38;5;241m.\u001b[39mskip_api:\n\u001b[0;32m--> 934\u001b[0m     block_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_info\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mapi_info()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(block, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_info_as_input\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    936\u001b[0m         block_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_info_as_input\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mapi_info_as_input()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gradio/components/base.py:335\u001b[0m, in \u001b[0;36mComponent.api_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mThe typing information for this component as a dictionary whose values are a list of 2 strings: [Python type, language-agnostic description].\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03mKeys of the dictionary are: raw_input, raw_output, serialized_input, serialized_output\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_model\u001b[38;5;241m.\u001b[39mmodel_json_schema()\n\u001b[1;32m    336\u001b[0m     desc \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    337\u001b[0m     schema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_description\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m desc\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/main.py:450\u001b[0m, in \u001b[0;36mBaseModel.model_json_schema\u001b[0;34m(cls, by_alias, ref_template, schema_generator, mode)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_json_schema\u001b[39m(\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m     mode: JsonSchemaMode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    437\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generates a JSON schema for a model class.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m        The JSON schema for the given model class.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_json_schema(\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28mcls\u001b[39m, by_alias\u001b[38;5;241m=\u001b[39mby_alias, ref_template\u001b[38;5;241m=\u001b[39mref_template, schema_generator\u001b[38;5;241m=\u001b[39mschema_generator, mode\u001b[38;5;241m=\u001b[39mmode\n\u001b[1;32m    452\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/json_schema.py:2255\u001b[0m, in \u001b[0;36mmodel_json_schema\u001b[0;34m(cls, by_alias, ref_template, schema_generator, mode)\u001b[0m\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility function to generate a JSON Schema for a model.\u001b[39;00m\n\u001b[1;32m   2238\u001b[0m \n\u001b[1;32m   2239\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;124;03m    The generated JSON Schema.\u001b[39;00m\n\u001b[1;32m   2252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2253\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m-> 2255\u001b[0m schema_generator_instance \u001b[38;5;241m=\u001b[39m schema_generator(by_alias\u001b[38;5;241m=\u001b[39mby_alias, ref_template\u001b[38;5;241m=\u001b[39mref_template)\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_core_schema__, _mock_val_ser\u001b[38;5;241m.\u001b[39mMockCoreSchema):\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_core_schema__\u001b[38;5;241m.\u001b[39mrebuild()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/json_schema.py:293\u001b[0m, in \u001b[0;36mGenerateJsonSchema.__init__\u001b[0;34m(self, by_alias, ref_template)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collision_counter: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collision_index: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_type_to_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_schema_type_to_method()\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# When we encounter definitions we need to try to build them immediately\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# so that they are available schemas that reference them\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# But it's possible that CoreSchema was never going to be used\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# (e.g. because the CoreSchema that references short circuits is JSON schema generation without needing\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m#  the reference) so instead of failing altogether if we can't build a definition we\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# store the error raised and re-throw it if we end up needing that def\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_core_defs_invalid_for_json_schema: \u001b[38;5;28mdict\u001b[39m[DefsRef, PydanticInvalidForJsonSchema] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pydantic/json_schema.py:338\u001b[0m, in \u001b[0;36mGenerateJsonSchema.build_schema_type_to_method\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m         mapping[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_name)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    339\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo method for generating JsonSchema for core_schema.type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    340\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    341\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping\n",
      "\u001b[0;31mTypeError\u001b[0m: No method for generating JsonSchema for core_schema.type='invalid' (expected: GenerateJsonSchema.invalid_schema)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Parameters\n",
    "    max_seq_len = 200  # Increased sequence length\n",
    "    embedding_dim = 100\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = load_data(max_seq_len)\n",
    "    (X_train, X_test, X_mlp_train, X_mlp_test, \n",
    "     y_train, y_test, vocab, le, tfidf, svd, scaler) = prepare_data(df, max_seq_len)\n",
    "    \n",
    "    # Load GloVe embeddings\n",
    "    print(\"Loading GloVe embeddings...\")\n",
    "    glove_path = '../glove.6B/glove.6B.100d.txt'  # Update with your path\n",
    "    glove = load_glove_embeddings(glove_path, vocab, embedding_dim)\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    print(\"Starting hyperparameter tuning...\")\n",
    "    tuner = kt.RandomSearch(\n",
    "        lambda hp: build_hybrid_model(hp, len(vocab), glove, embedding_dim, max_seq_len),\n",
    "        objective='val_accuracy',\n",
    "        max_trials=10,\n",
    "        executions_per_trial=1,\n",
    "        directory='tuner_dir',\n",
    "        project_name='mbti_hybrid_enhanced'\n",
    "    )\n",
    "    \n",
    "    tuner.search(\n",
    "        [X_train, X_mlp_train],\n",
    "        y_train,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[\n",
    "            EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(factor=0.1, patience=3)\n",
    "        ],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = best_model.evaluate([X_test, X_mlp_test], y_test)\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Save model and assets\n",
    "    best_model.save('mbti_enhanced_model.h5')\n",
    "    with open('label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(le, f)\n",
    "    with open('vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    with open('tfidf.pkl', 'wb') as f:\n",
    "        pickle.dump(tfidf, f)\n",
    "    with open('svd.pkl', 'wb') as f:\n",
    "        pickle.dump(svd, f)\n",
    "    with open('scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    # Gradio interface\n",
    "    def predict(text):\n",
    "        # Clean and tokenize text\n",
    "        cleaned = enhanced_clean_text(text)\n",
    "        tokens = advanced_tokenization(cleaned)[:max_seq_len]\n",
    "        sequence = [vocab.get(token, 0) for token in tokens]\n",
    "        padded_seq = np.array([sequence + [0]*(max_seq_len - len(sequence)) or sequence])\n",
    "        \n",
    "        # Prepare MLP features\n",
    "        tfidf_features = tfidf.transform([cleaned])\n",
    "        svd_features = svd.transform(tfidf_features)\n",
    "        \n",
    "        # Additional features\n",
    "        sentiment = TextBlob(cleaned).sentiment.polarity\n",
    "        readability = flesch_reading_ease(cleaned)\n",
    "        word_count = len(cleaned.split())\n",
    "        char_count = len(cleaned)\n",
    "        additional_features = np.array([[sentiment, readability, word_count, char_count]])\n",
    "        \n",
    "        # Combine and scale features\n",
    "        mlp_features = np.concatenate([svd_features, additional_features], axis=1)\n",
    "        mlp_features = scaler.transform(mlp_features)\n",
    "        \n",
    "        # Predict\n",
    "        proba = best_model.predict([padded_seq, mlp_features], verbose=0)[0]\n",
    "        return {le.classes_[i]: float(proba[i]) for i in range(16)}\n",
    "    \n",
    "    # Gradio interface\n",
    "    iface = gr.Interface(\n",
    "        fn=predict,\n",
    "        inputs=gr.Textbox(label=\"Enter your text\"),\n",
    "        outputs=gr.Label(num_top_classes=4),\n",
    "        title=\"Enhanced MBTI Personality Predictor\",\n",
    "        description=\"This model uses advanced NLP techniques to predict MBTI personality types from text.\"\n",
    "    )\n",
    "    iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
